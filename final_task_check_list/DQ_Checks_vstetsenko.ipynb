{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a4e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder\\\n",
    "    .appName(\"SparkSession\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9759a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_df_read(path):\n",
    "    '''function to choose read method'''\n",
    "    if path.endswith('csv'):\n",
    "        df = spark.read.options(header='True', inferSchema='True', delimiter=',').csv(path)\n",
    "    else:\n",
    "        df = spark.read.parquet(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "232d8505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "pd.set_option(\"display.precision\", 11)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def func_completeness_by_sum(table, path_source, path_target, list_columns, list_pk):\n",
    "    '''if column name differs from source provide columns names as a tuple where first name from source and second from target''' \n",
    "    \n",
    "    source_df_spark = choose_df_read(path_source)\n",
    "    target_df_spark = choose_df_read(path_target)\n",
    "    \n",
    "\n",
    "    \n",
    "    result_list = []\n",
    "    \n",
    "    for column in list_columns:\n",
    "        if isinstance(column, tuple):\n",
    "            source_df_spark = source_df_spark.withColumn(column[0],col(column[0]).cast('double'))\n",
    "            source_df = source_df_spark.toPandas()\n",
    "            \n",
    "            target_df_spark = target_df_spark.withColumn(column[1],col(column[1]).cast('double'))\n",
    "            target_df = target_df_spark.toPandas()\n",
    "            \n",
    "            sum_source = source_df[column[0]].sum()\n",
    "            sum_target = target_df[column[1]].sum()\n",
    "        else:\n",
    "            source_df_spark = source_df_spark.withColumn(column,col(column).cast('double'))\n",
    "            source_df = source_df_spark.toPandas()\n",
    "            \n",
    "            target_df_spark = target_df_spark.withColumn(column,col(column).cast('double'))\n",
    "            target_df = target_df_spark.toPandas()\n",
    "            \n",
    "            sum_source = source_df[column].sum()\n",
    "            sum_target = target_df[column].sum()\n",
    "        \n",
    "        bad_data = []\n",
    "        result_check = sum_source == sum_target\n",
    "        if sum_source != sum_target:\n",
    "            merged_df = pd.merge(source_df, target_df, on=list_pk, suffixes=('_source', '_target'))\n",
    "            if isinstance(column, tuple):\n",
    "                name_source = column[0]\n",
    "                name_target = column[1]\n",
    "            else:\n",
    "                name_source = column + '_source'\n",
    "                name_target = column + '_target'\n",
    "            filtered_df = merged_df[merged_df[name_source] != merged_df[name_target]]\n",
    "            list_to_displ = list_pk\n",
    "            list_to_displ.append(name_source)\n",
    "            list_to_displ.append(name_target)\n",
    "            bad_data = filtered_df[list_to_displ].values.tolist()\n",
    "        \n",
    "        result_list.append([table, 'Completeness', column, result_check, bad_data])\n",
    "        \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5928373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Airports', 'Completeness', 'lat', True, []], ['Airports', 'Completeness', ('long', 'longt'), True, []]]\n"
     ]
    }
   ],
   "source": [
    "print(func_completeness_by_sum('Airports', \"source/airports.csv\", \"raw/airports/*.parquet\",  ['lat', ('long', 'longt')], ['iata', 'airport'] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c62cee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, col, substring, lpad, when\n",
    "\n",
    "def func_validity_by_time_range(table, path_to_data, list_columns):\n",
    "    \n",
    "    df = choose_df_read(path_to_data)\n",
    "\n",
    "    result_list = []\n",
    "    \n",
    "    for column in list_columns:\n",
    "        \n",
    "        modified_df = df.withColumn(\n",
    "                                            column,\n",
    "                                            when((col(column).isNull()) | (col(column) == ''), None)\n",
    "                                            .otherwise(lpad(col(column), 4, '0'))\n",
    "                                            )\n",
    "        invalid_data = modified_df.filter(\n",
    "            ~((substring(col(column), 1, 2).cast('integer').between(0, 23)) &\n",
    "              (substring(col(column), 3, 4).cast('integer').between(0, 59)))\n",
    "            ).select(column)\n",
    "        \n",
    "        result_check = True if invalid_data.count() == 0 else False\n",
    "        \n",
    "        list_bad_data = invalid_data.select(column).rdd.flatMap(lambda x: x).collect()\n",
    "        \n",
    "        result_list.append([table, 'Validity', column, result_check, list_bad_data])\n",
    "    \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7672ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueness_by_PK(table, path_to_data, list_columns, flag_combination = False):\n",
    "    \n",
    "    df = choose_df_read(path_to_data)\n",
    "        \n",
    "    result_list = []\n",
    "    if flag_combination:\n",
    "        unique_by_combintation = df.groupBy(list_columns).count()\n",
    "        non_unique = unique_by_combintation.filter(col('count') > 1)\n",
    "        result_check = True if non_unique.count() == 0 else False\n",
    "        list_bad_data = [list(row) for row in non_unique.select(list_columns).collect()]\n",
    "        result_list.append([table, 'Uniqueness by combination', list_columns, result_check, list_bad_data])\n",
    "    else: \n",
    "        # check by uniqueness by combination of PK\n",
    "        for column in list_columns:\n",
    "            unique = df.groupBy(column).count()\n",
    "            non_unique = unique.filter(col('count') > 1)\n",
    "            result_check = True if non_unique.count() == 0 else False\n",
    "        \n",
    "            list_bad_data = non_unique.select(column).rdd.flatMap(lambda x: x).collect()\n",
    "        \n",
    "            result_list.append([table, 'Uniqueness', column, result_check, list_bad_data])\n",
    "            \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "487c4a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consistency_for_CancellationCode(table, path_to_data):\n",
    "    \n",
    "    df = choose_df_read(path_to_data)\n",
    "        \n",
    "    result_list = []\n",
    "    inconsistent_cancellation = df.filter(\n",
    "        (col('Cancelled') == 0) & ~col('CancellationCode').isin('') |\n",
    "        (col('Cancelled') == 1) & ~col('CancellationCode').isin('A', 'B', 'C')\n",
    "    )\n",
    "    \n",
    "    result_check = True if inconsistent_cancellation.count() == 0 else False\n",
    "    list_bad_data = [list(row) for row in inconsistent_cancellation.select('Cancelled', 'CancellationCode').collect()]\n",
    "    result_list.append([table, 'Consistency check for CancellationCode', 'CancellationCode', result_check, list_bad_data])\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af8c12ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/25 07:23:59 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_list = []\n",
    "result_list.append(func_completeness_by_sum('Airports', \"source/airports.csv\", \"raw/airports/*.parquet\",  ['lat', ('long', 'longt')], ['iata', 'airport'] ))\n",
    "result_list.append(func_validity_by_time_range('Flights', \"raw/flights/*.parquet\", ['ArrTime', 'DepTime']))\n",
    "result_list.append(uniqueness_by_PK('Carrier', \"raw/carriers/*.parquet\", ['Code', 'Description']))\n",
    "result_list.append(uniqueness_by_PK('Carrier', \"raw/carriers/*.parquet\", ['Code', 'Description'], True))\n",
    "result_list.append(consistency_for_CancellationCode('Flights', \"raw/flights/*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cdad218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Table                                DQ check               Column  \\\n",
      "#                                                                          \n",
      "1  Airports                            Completeness                  lat   \n",
      "2  Airports                            Completeness        (long, longt)   \n",
      "3   Flights                                Validity              ArrTime   \n",
      "4   Flights                                Validity              DepTime   \n",
      "5   Carrier                              Uniqueness                 Code   \n",
      "6   Carrier                              Uniqueness          Description   \n",
      "7   Carrier               Uniqueness by combination  [Code, Description]   \n",
      "8   Flights  Consistency check for CancellationCode     CancellationCode   \n",
      "\n",
      "   Status  \\\n",
      "#           \n",
      "1    True   \n",
      "2    True   \n",
      "3   False   \n",
      "4   False   \n",
      "5   False   \n",
      "6   False   \n",
      "7   False   \n",
      "8   False   \n",
      "\n",
      "                                                                            Bad Data  \n",
      "#                                                                                     \n",
      "1                                                                                 []  \n",
      "2                                                                                 []  \n",
      "3                                                                             [2926]  \n",
      "4                                                                             [2500]  \n",
      "5                                                              [16, ZUQ, 0BQ, 07Q, ]  \n",
      "6  [DCA, Zuliana De Aviacion, Flair Airlines Ltd., Smokey Bay Air Inc., , Regal Air]  \n",
      "7               [[07Q, Flair Airlines Ltd.], [0BQ, DCA], [ZUQ, Zuliana De Aviacion]]  \n",
      "8                                                   [[0, A], [0, B], [0, C], [1, E]]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "flattened_list = []\n",
    "for sublist in result_list:\n",
    "    for item in sublist:\n",
    "        flattened_list.append(item)\n",
    "\n",
    "df = pd.DataFrame(flattened_list, columns=['Table', 'DQ check', 'Column', 'Status', 'Bad Data'])\n",
    "\n",
    "df.insert(0, '#', range(1, len(df) + 1))\n",
    "\n",
    "df.set_index('#', inplace=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c80e7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local PySpark (Python-3.7.9 / Spark-3.0.1 )",
   "language": "python",
   "name": "py3spark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
