{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a4e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder\\\n",
    "    .appName(\"SparkSession\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afed3bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_df_read(path):\n",
    "    '''function to choose read method'''\n",
    "    if path.endswith('csv'):\n",
    "        df = spark.read.options(header='True', inferSchema='True', delimiter=',').csv(path)\n",
    "    else:\n",
    "        df = spark.read.parquet(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "232d8505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "def func_completeness_by_sum(table, path_source, path_target, list_columns):\n",
    "    '''if column name differs from source provide columns names as a tuple where first name from source and second from target''' \n",
    "    source_df = choose_df_read(path_source)\n",
    "    target_df = choose_df_read(path_target)\n",
    "    \n",
    "    result_list = []\n",
    "    \n",
    "    for column in list_columns:\n",
    "        \n",
    "        if isinstance(column, tuple): \n",
    "            sum_source = source_df.select(sum(column[0])).collect()[0][0]\n",
    "            sum_target = target_df.select(sum(column[1])).collect()[0][0]\n",
    "        else: \n",
    "            sum_source = source_df.select(sum(column)).collect()[0][0]\n",
    "            sum_target = target_df.select(sum(column)).collect()[0][0]\n",
    "            \n",
    "        result_check = True if sum_source == sum_target else False\n",
    "        \n",
    "        result_list.append([table, 'Completeness', column, result_check, [sum_source, sum_target]])\n",
    "        \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c62cee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, substring, lpad, when\n",
    "\n",
    "def func_validity_by_time_range(table, path_to_data, list_columns):\n",
    "    \n",
    "    df = choose_df_read(path_to_data)\n",
    "\n",
    "    result_list = []\n",
    "    \n",
    "    for column in list_columns:\n",
    "        \n",
    "        modified_df = df.withColumn(\n",
    "                                            column,\n",
    "                                            when((col(column).isNull()) | (col(column) == ''), None)\n",
    "                                            .otherwise(lpad(col(column), 4, '0'))\n",
    "                                            )\n",
    "        invalid_data = modified_df.filter(\n",
    "            ~((substring(col(column), 1, 2).cast('integer').between(0, 23)) &\n",
    "              (substring(col(column), 3, 4).cast('integer').between(0, 59)))\n",
    "            ).select(column)\n",
    "        \n",
    "        result_check = True if invalid_data.count() == 0 else False\n",
    "        \n",
    "        list_bad_data = invalid_data.select(column).rdd.flatMap(lambda x: x).collect()\n",
    "        \n",
    "        result_list.append([table, 'Validity', column, result_check, list_bad_data])\n",
    "    \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a473ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueness_by_PK(table, path_to_data, list_columns, flag_combination = False):\n",
    "    \n",
    "    df = choose_df_read(path_to_data)\n",
    "        \n",
    "    result_list = []\n",
    "    if flag_combination:\n",
    "        unique_by_combintation = df.groupBy(list_columns).count()\n",
    "        non_unique = unique_by_combintation.filter(col('count') > 1)\n",
    "        result_check = True if non_unique.count() == 0 else False\n",
    "        list_bad_data = [list(row) for row in non_unique.select(list_columns).collect()]\n",
    "        result_list.append([table, 'Uniqueness by combination', list_columns, result_check, list_bad_data])\n",
    "    else: \n",
    "        # check by uniqueness by combination of PK\n",
    "        for column in list_columns:\n",
    "            unique = df.groupBy(column).count()\n",
    "            non_unique = unique.filter(col('count') > 1)\n",
    "            result_check = True if non_unique.count() == 0 else False\n",
    "        \n",
    "            list_bad_data = non_unique.select(column).rdd.flatMap(lambda x: x).collect()\n",
    "        \n",
    "            result_list.append([table, 'Uniqueness', column, result_check, list_bad_data])\n",
    "            \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "545c38be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consistency_for_CancellationCode(table, path_to_data):\n",
    "    \n",
    "    df = choose_df_read(path_to_data)\n",
    "        \n",
    "    result_list = []\n",
    "    inconsistent_cancellation = df.filter(\n",
    "        (col('Cancelled') == 0) & ~col('CancellationCode').isin('') |\n",
    "        (col('Cancelled') == 1) & ~col('CancellationCode').isin('A', 'B', 'C')\n",
    "    )\n",
    "    \n",
    "    result_check = True if inconsistent_cancellation.count() == 0 else False\n",
    "    list_bad_data = [list(row) for row in inconsistent_cancellation.select('Cancelled', 'CancellationCode').collect()]\n",
    "    result_list.append([table, 'Consistency check for CancellationCode', 'CancellationCode', result_check, list_bad_data])\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4950e540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_list = []\n",
    "result_list.append(func_completeness_by_sum('Airports', \"source/airports.csv\", \"raw/airports/*.parquet\",  ['lat', ('long', 'longt')], ))\n",
    "result_list.append(func_validity_by_time_range('Flights', \"raw/flights/*.parquet\", ['ArrTime', 'DepTime']))\n",
    "result_list.append(uniqueness_by_PK('Carrier', \"raw/carriers/*.parquet\", ['Code', 'Description']))\n",
    "result_list.append(uniqueness_by_PK('Carrier', \"raw/carriers/*.parquet\", ['Code', 'Description'], True))\n",
    "result_list.append(consistency_for_CancellationCode('Flights', \"raw/flights/*.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c759c809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Table                                DQ check               Column  \\\n",
      "#                                                                          \n",
      "1  Airports                            Completeness                  lat   \n",
      "2  Airports                            Completeness        (long, longt)   \n",
      "3   Flights                                Validity              ArrTime   \n",
      "4   Flights                                Validity              DepTime   \n",
      "5   Carrier                              Uniqueness                 Code   \n",
      "6   Carrier                              Uniqueness          Description   \n",
      "7   Carrier               Uniqueness by combination  [Code, Description]   \n",
      "8   Flights  Consistency check for CancellationCode     CancellationCode   \n",
      "\n",
      "   Status                                           Bad Data  \n",
      "#                                                             \n",
      "1   False           [135571.32519867975, 135571.32519867996]  \n",
      "2   False          [-333837.7226786295, -333837.72267863044]  \n",
      "3   False                                             [2926]  \n",
      "4   False                                             [2500]  \n",
      "5   False                              [16, ZUQ, 0BQ, 07Q, ]  \n",
      "6   False  [DCA, Zuliana De Aviacion, Flair Airlines Ltd....  \n",
      "7   False  [[07Q, Flair Airlines Ltd.], [0BQ, DCA], [ZUQ,...  \n",
      "8   False                   [[0, A], [0, B], [0, C], [1, E]]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "flattened_list = []\n",
    "for sublist in result_list:\n",
    "    for item in sublist:\n",
    "        flattened_list.append(item)\n",
    "\n",
    "df = pd.DataFrame(flattened_list, columns=['Table', 'DQ check', 'Column', 'Status', 'Bad Data'])\n",
    "\n",
    "df.insert(0, '#', range(1, len(df) + 1))\n",
    "\n",
    "df.set_index('#', inplace=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357ff6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local PySpark (Python-3.7.9 / Spark-3.0.1 )",
   "language": "python",
   "name": "py3spark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
